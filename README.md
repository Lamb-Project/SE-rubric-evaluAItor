# Sistema de Evaluaci√≥n Automatizada de Proyectos

Un sistema automatizado para evaluar proyectos acad√©micos de Ingenier√≠a del Software utilizando modelos de lenguaje (LLMs). El sistema analiza documentos de proyectos y genera evaluaciones estructuradas basadas en una r√∫brica predefinida.

## üöÄ Caracter√≠sticas Principales

- **Evaluaci√≥n Automatizada**: Procesamiento completo de documentos con an√°lisis detallado
- **M√∫ltiples Proveedores**: Soporte para Anthropic (Claude), OpenAI y Ollama
- **An√°lisis en 4 Fases**: Extracci√≥n ‚Üí Coherencia ‚Üí Evaluaci√≥n ‚Üí S√≠ntesis
- **R√∫brica Completa**: 5 criterios con pesos espec√≠ficos (Objetivos 20%, Requisitos Info 15%, Req. No Func. 10%, Casos Uso 35%, Matrices 20%)
- **Salida Markdown**: Reportes estructurados y legibles
- **Logging Detallado**: Seguimiento completo del proceso de evaluaci√≥n

## üìã Requisitos del Sistema

- **Python**: 3.11 o superior
- **Dependencias**: Ver `requirements.txt`
- **API Keys**: Dependiendo del proveedor seleccionado

## üõ†Ô∏è Instalaci√≥n

1. **Clona o descarga el proyecto**
   ```bash
   cd prototipo-2
   ```

2. **Instala las dependencias**
   ```bash
   # Instalar dependencias b√°sicas (Anthropic + utilidades)
   pip install -r requirements.txt

   # O instalar todo (incluyendo OpenAI) de una vez
   pip install -r requirements_all.txt
   ```

3. **Para soporte OpenAI (opcional, si no usaste requirements_all.txt)**
   ```bash
   pip install openai>=1.0.0
   ```

## üìñ Uso B√°sico

### Sintaxis General
```bash
python evaluador.py <archivo_documento> <directorio_salida> [opciones]
```

### Par√°metros Obligatorios
- `archivo_documento`: Ruta al documento del proyecto a evaluar (formato texto/Markdown)
- `directorio_salida`: Directorio donde se guardar√°n los resultados

### Opciones Principales
- `--prompts <directorio>`: Directorio con archivos de prompts (por defecto: `./prompts`)
- `--api-key <key>`: API key (o usar variables de entorno)
- `--ollama <modelo>`: Usar Ollama con el modelo especificado
- `--openai <modelo>`: Usar OpenAI con el modelo especificado
- `--debug`: Modo debug con informaci√≥n detallada

## üåê Proveedores Soportados

### Anthropic (Claude) - Por Defecto
```bash
# Usando variable de entorno
export ANTHROPIC_API_KEY=sk-ant-...
python evaluador.py documento.md ./resultados

# Usando par√°metro
python evaluador.py documento.md ./resultados --api-key sk-ant-...
```

### Ollama (Local)
```bash
# Instalar y ejecutar Ollama primero
ollama pull mistral:latest
ollama serve

# Usar en el evaluador
python evaluador.py documento.md ./resultados --ollama mistral:latest
```

### OpenAI (GPT)
```bash
# Instalar dependencia adicional
pip install openai

# Usar GPT-4
export OPENAI_API_KEY=sk-...
python evaluador.py documento.md ./resultados --openai gpt-4-turbo-preview

# Usar GPT-3.5 (m√°s econ√≥mico)
python evaluador.py documento.md ./resultados --openai gpt-3.5-turbo
```

## üìä Proceso de Evaluaci√≥n

El sistema ejecuta 4 fases secuenciales:

### Fase 1: Extracci√≥n de Informaci√≥n
- **1.1**: Extracci√≥n de objetivos del proyecto
- **1.2**: Extracci√≥n de requisitos (funcionales y no funcionales)
- **1.3**: Extracci√≥n de casos de uso

### Fase 2: An√°lisis de Coherencia
- **2.1**: An√°lisis de trazabilidad entre elementos
- **2.2**: An√°lisis de completitud de requisitos

### Fase 3: Evaluaci√≥n por Criterios
- **3.1**: Evaluaci√≥n de objetivos (20% del peso total)
- **3.2**: Evaluaci√≥n de requisitos de informaci√≥n (15%)
- **3.3**: Evaluaci√≥n de requisitos no funcionales (10%)
- **3.4**: Evaluaci√≥n de casos de uso (35%)
- **3.5**: Evaluaci√≥n de matrices de trazabilidad (20%)

### Fase 4: S√≠ntesis y Calificaci√≥n Final
- Generaci√≥n del informe consolidado
- C√°lculo de nota ponderada (0-10)
- Documento final con r√∫brica completa

!(docs/Evaluaitor.png)[]

## üìÅ Estructura de Archivos Generados

```
<directorio_salida>/
‚îú‚îÄ‚îÄ evaluacion.log              # Log detallado del proceso
‚îú‚îÄ‚îÄ 1_1_objetivos.md           # Objetivos extra√≠dos
‚îú‚îÄ‚îÄ 1_2_requisitos.md          # Requisitos extra√≠dos
‚îú‚îÄ‚îÄ 1_3_casos_uso.md           # Casos de uso extra√≠dos
‚îú‚îÄ‚îÄ 2_1_trazabilidad.md        # An√°lisis de trazabilidad
‚îú‚îÄ‚îÄ 2_2_completitud.md         # An√°lisis de completitud
‚îú‚îÄ‚îÄ 3_1_eval_objetivos.md      # Evaluaci√≥n de objetivos
‚îú‚îÄ‚îÄ 3_2_eval_requisitos_info.md # Evaluaci√≥n req. info
‚îú‚îÄ‚îÄ 3_3_eval_requisitos_nf.md  # Evaluaci√≥n req. no func.
‚îú‚îÄ‚îÄ 3_4_eval_casos_uso.md      # Evaluaci√≥n casos de uso
‚îú‚îÄ‚îÄ 3_5_eval_matrices.md       # Evaluaci√≥n matrices
‚îî‚îÄ‚îÄ EVALUACION_FINAL.md        # Informe consolidado final
```

## üí∞ Costos Estimados

| Proveedor | Modelo | Costo por 1k tokens | Notas |
|-----------|--------|-------------------|-------|
| Anthropic | Claude 3 Sonnet | $0.003 entrada / $0.015 salida | Alta calidad (por defecto) |
| OpenAI | GPT-4 Turbo | $0.01 entrada / $0.03 salida | Muy alta calidad |
| OpenAI | GPT-3.5 Turbo | $0.0005 entrada / $0.0015 salida | Buena calidad, econ√≥mico |
| Ollama | Cualquier modelo | Gratuito | Calidad variable, ejecuci√≥n local |

**Costo estimado por evaluaci√≥n completa**: ~$0.50-2.00 USD (dependiendo del proveedor y tama√±o del documento)

## üîß Configuraci√≥n Avanzada

### Variables de Entorno
```bash
# Anthropic
export ANTHROPIC_API_KEY=sk-ant-...

# OpenAI
export OPENAI_API_KEY=sk-...

# Ollama (opcional, por defecto localhost:11434)
export OLLAMA_BASE_URL=http://localhost:11434
```

### Personalizaci√≥n de Prompts
Los prompts se almacenan en el directorio `./prompts/` y pueden ser modificados para:
- Ajustar criterios de evaluaci√≥n
- Cambiar el idioma de evaluaci√≥n
- Modificar el formato de salida

### Configuraci√≥n del Sistema
- **L√≠mite de tokens**: M√°ximo 4096 tokens por llamada API
- **Tiempo estimado**: 2-5 minutos por evaluaci√≥n completa
- **Tama√±o m√°ximo**: Documentos hasta 10MB (recomendado < 50KB)

## üß™ Testing y Validaci√≥n

### Probar Proveedores
```bash
python test_providers.py
```

### Documento de Prueba
Se incluye `test_doc.md` para pruebas r√°pidas del sistema.

## üìã Ejemplos de Uso

### Evaluaci√≥n B√°sica con Claude
```bash
python evaluador.py proyecto_final.md ./evaluacion_proyecto
```

### Evaluaci√≥n con Modelo Local (Ollama)
```bash
python evaluador.py proyecto_final.md ./evaluacion_proyecto --ollama mistral:latest
```

### Evaluaci√≥n con OpenAI GPT-4
```bash
python evaluador.py proyecto_final.md ./evaluacion_proyecto --openai gpt-4-turbo-preview
```

### Evaluaci√≥n con Prompts Personalizados
```bash
python evaluador.py proyecto_final.md ./evaluacion_proyecto --prompts ./mis_prompts
```

### Modo Debug
```bash
python evaluador.py proyecto_final.md ./evaluacion_proyecto --debug
```

## üåä Versi√≥n Streaming (Recomendada)

La versi√≥n streaming (`evaluador_streaming.py`) ofrece mejoras significativas sobre la versi√≥n est√°ndar:

### üöÄ Caracter√≠sticas de la Versi√≥n Streaming

- **Streaming activado** para Anthropic Claude, Ollama y OpenAI
- **Timeouts extendidos**: 15 minutos (vs 5 minutos en la versi√≥n est√°ndar)
- **Reintentos aumentados**: Hasta 5 reintentos (vs 3 en la versi√≥n est√°ndar)
- **Mejor feedback** durante la evaluaci√≥n con actualizaciones en tiempo real
- **Menor uso de memoria** gracias al procesamiento por fragmentos
- **Fallback autom√°tico** si el streaming falla

### üõ†Ô∏è Uso de la Versi√≥n Streaming

#### Con Anthropic Claude (Streaming activado)
```bash
python evaluador_streaming.py documento.md ./resultados
# O con API key expl√≠cita
python evaluador_streaming.py documento.md ./resultados --api-key sk-ant-...
```

#### Con Ollama (Streaming activado)
```bash
python evaluador_streaming.py documento.md ./resultados --ollama llama2
python evaluador_streaming.py documento.md ./resultados --ollama mistral:latest
```

#### Con OpenAI (Streaming activado)
```bash
python evaluador_streaming.py documento.md ./resultados --openai gpt-4-turbo-preview
python evaluador_streaming.py documento.md ./resultados --openai gpt-3.5-turbo
```

#### Procesamiento por Lotes (Directorio)
```bash
python evaluador_streaming.py --input-dir ./documentos ./resultados
```

#### Opciones Avanzadas
```bash
# Modo debug con informaci√≥n detallada
python evaluador_streaming.py documento.md ./resultados --debug

# Con prompts personalizados
python evaluador_streaming.py documento.md ./resultados --prompts ./mis_prompts
```

### üìä Comparaci√≥n de Versiones

| Caracter√≠stica | Versi√≥n Est√°ndar | Versi√≥n Streaming |
|----------------|------------------|-------------------|
| Streaming Ollama | ‚ùå Desactivado | ‚úÖ **Activado** |
| Streaming Anthropic | ‚ùå No disponible | ‚úÖ **Activado** |
| Streaming OpenAI | ‚ùå No disponible | ‚úÖ **Activado** |
| Timeout m√°ximo | 5 minutos | **15 minutos** |
| Reintentos | 3 | **5** |
| Uso de memoria | Alto | **Optimizado** |
| Feedback | B√°sico | **En tiempo real** |

### üí° Recomendaci√≥n

**Use la versi√≥n streaming para:**
- Documentos largos que requieren m√°s tiempo de procesamiento
- Mejor experiencia de usuario con feedback en tiempo real
- Procesamiento m√°s eficiente de memoria
- Mayor estabilidad con reintentos autom√°ticos

## üö® Soluci√≥n de Problemas

### Errores Comunes

**Archivo no encontrado**
```
‚ùå Error: El archivo 'documento.md' no existe
```
*Soluci√≥n*: Verificar la ruta del archivo de entrada

**API Key no configurada**
```
‚ùå Error: No se puede conectar con la API
```
*Soluci√≥n*: Configurar API key en variable de entorno o par√°metro `--api-key`

**Ollama no ejecut√°ndose**
```
‚ùå Error: No se puede conectar con Ollama
```
*Soluci√≥n*: Ejecutar `ollama serve` y verificar que el modelo est√© descargado

**Directorio de prompts faltante**
```
‚ùå Error: El directorio de prompts no existe
```
*Soluci√≥n*: Verificar que existe el directorio `./prompts/` con todos los archivos requeridos

### Logs y Depuraci√≥n
- Usar `--debug` para informaci√≥n detallada
- Revisar `evaluacion.log` en el directorio de salida
- Verificar conectividad de red para proveedores en la nube

## üìö Documentaci√≥n Adicional

- [PRD - Product Requirements Document](docs/prd.md)
- [Soporte Multi-Proveedor](README_PROVIDERS.md)
- [Arquitectura del Sistema](docs/) - M√°s documentaci√≥n t√©cnica

## ü§ù Contribuci√≥n

Para contribuir al proyecto:
1. Revisar la documentaci√≥n en `docs/`
2. Seguir la estructura modular del c√≥digo
3. A√±adir tests para nuevas funcionalidades
4. Actualizar documentaci√≥n seg√∫n cambios

## üìÑ Licencia

Sistema de Evaluaci√≥n Automatizada de Proyectos - Un sistema automatizado para evaluar proyectos acad√©micos de Ingenier√≠a del Software utilizando modelos de lenguaje (LLMs).

Copyright (C) 2025 Marc Alier, Francisco Garcia-Pe√±alvo, Alicia Garcia-Holgado, Andrea V√°zquez Ingelmo, Maria Jos√© Casa√±, Juanan Pereira

Este proyecto es parte de un trabajo acad√©mico para la evaluaci√≥n automatizada de proyectos de Ingenier√≠a del Software.

This program is free software: you can redistribute it and/or modify
it under the terms of the GNU General Public License as published by
the Free Software Foundation, either version 3 of the License, or
(at your option) any later version.

This program is distributed in the hope that it will be useful,
but WITHOUT ANY WARRANTY; without even the implied warranty of
MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
GNU General Public License for more details.

You should have received a copy of the GNU General Public License
along with this program.  If not, see <https://www.gnu.org/licenses/>.

## ¬© Copyright

**Autores:**
- Marc Alier
- Francisco Garcia-Pe√±alvo
- Alicia Garcia-Holgado
- Andrea V√°zquez Ingelmo
- Maria Jos√© Casa√±
- Juanan Pereira

**Universidades:**
- Universitat Polit√®cnica de Catalunya (UPC)
- Universidad de Salamanca (USAL)
- Universitat Polit√®cnica de Val√®ncia (UPV)

---

**Versi√≥n**: Prototipo 2.0
**Fecha**: Noviembre 2025
**Autor**: Sistema de Evaluaci√≥n Automatizada con LLM
